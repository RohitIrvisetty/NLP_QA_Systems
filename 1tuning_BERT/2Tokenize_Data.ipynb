{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_5y79RH-zWg"
      },
      "source": [
        "# Tokenize the data (Contexts, questions, answer start and end index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHddetUEHMl6",
        "outputId": "6870cf45-dfc5-422f-9635-ddedb3314a1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.27.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.24.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.3.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers) (2.22.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "import torch\n",
        "import json\n",
        "import json\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "lW30VsFRMCOg"
      },
      "outputs": [],
      "source": [
        "training_contexts, training_questions, training_ans = [], [], []\n",
        "validation_contexts, validation_questions, validation_ans = [], [], []\n",
        "\n",
        "with open('SQUAD2.0_data/modified/train_contexts-modified.json', 'rb') as f:\n",
        "    training_contexts = json.load(f)\n",
        "\n",
        "with open('SQUAD2.0_data/modified/train_ques-modified.json', 'rb') as f:\n",
        "    training_questions = json.load(f)\n",
        "\n",
        "with open('SQUAD2.0_data/modified/train_ans-modified.json', 'rb') as f:\n",
        "    training_ans = json.load(f)\n",
        "\n",
        "with open('SQUAD2.0_data/modified/dev_contexts-modified.json', 'rb') as f:\n",
        "    validation_contexts = json.load(f)\n",
        "\n",
        "with open('SQUAD2.0_data/modified/dev_ques-modified.json', 'rb') as f:\n",
        "    validation_questions = json.load(f)\n",
        "\n",
        "with open('SQUAD2.0_data/modified/dev_ans-modified.json', 'rb') as f:\n",
        "    validation_ans = json.load(f)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BERT base uncased"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 2.34kB/s]\n",
            "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 103kB/s]\n",
            "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.01MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 1.62MB/s]\n"
          ]
        }
      ],
      "source": [
        "### We will be using uncased model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenize validation contexts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_batch = tokenizer(training_contexts, training_questions, truncation=True, padding=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenize validation contexts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ezeMR8qMFMD3"
      },
      "outputs": [],
      "source": [
        "validation_batch = tokenizer(validation_contexts, validation_questions, truncation=True, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "OuDZ61asg0dT"
      },
      "outputs": [],
      "source": [
        "answer_start_positions, answer_end_positions = [], []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "72kTGU6el3vH"
      },
      "outputs": [],
      "source": [
        "default = tokenizer.model_max_length\n",
        "def update_start_end_tokens(batch, ans):\n",
        "  for i in range(len(ans)):\n",
        "    if batch.char_to_token(i, ans[i]['answer_start']) is not None:\n",
        "      answer_start_positions.append(batch.char_to_token(i, ans[i]['answer_start']))\n",
        "    else:\n",
        "      answer_start_positions.append(default)\n",
        "    if batch.char_to_token(i, ans[i]['answer_end']) is not None:\n",
        "      answer_end_positions.append(batch.char_to_token(i, ans[i]['answer_end']))\n",
        "    elif batch.char_to_token(i, ans[i]['answer_end']-1) is not None:\n",
        "      answer_end_positions.append(batch.char_to_token(i, ans[i]['answer_end']-1))\n",
        "    else:\n",
        "      answer_end_positions.append(default)\n",
        "  batch.update({'start_positions': answer_start_positions, 'end_positions': answer_end_positions})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9Gip70qvo4T4"
      },
      "outputs": [],
      "source": [
        "answer_start_positions, answer_end_positions = [], [] \n",
        "update_start_end_tokens(training_batch, training_ans)\n",
        "answer_start_positions, answer_end_positions = [], [] \n",
        "update_start_end_tokens(validation_batch, validation_ans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('SQUAD2.0_data/tokenized/training_tokenized.pickle', 'wb') as output:\n",
        "    pickle.dump(training_batch, output)\n",
        "\n",
        "with open('SQUAD2.0_data/tokenized/validation_tokenized.pickle', 'wb') as output:\n",
        "    pickle.dump(validation_batch, output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
